{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1 - scrape data\n",
    "In order to work, we require that the data is saved to a csv file with one piece of text (e.g. one paper title) on each line. There are no actual commas in the file, and there shouldn't be newlines contained in each piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scrape arxiv for titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/oai2?verb=ListRecords&from=2019-05-01&until=2019-05-02&metadataPrefix=arXiv&set=physics\n",
      "fetching up to  1000 records...\n",
      "fetching up to  2000 records...\n",
      "Got 503. Retrying after 10 seconds.\n",
      "fetching up to  2000 records...\n",
      "fetching is completed in 26.7 seconds.\n",
      "Total number of records 131\n"
     ]
    }
   ],
   "source": [
    "import arxivscraper as ax\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "# scraper for arxiv stat.ml\n",
    "scraper = ax.Scraper(category='stat', date_from='2017-08-01',\n",
    "                     date_until='2019-07-01', t=10, \n",
    "                     filters={'categories':['stat.ml'],'abstract':['learning']})\n",
    "\n",
    "# scraper for arxiv q-bio\n",
    "scraper = ax.Scraper(category='q-bio', date_from='2016-08-01',\n",
    "                     date_until='2019-07-01', t=10, \n",
    "                     filters={'categories':['q-bio.GN', 'q-bio.NC']})\n",
    "'''\n",
    "\n",
    "# scraper for arxiv physics\n",
    "scraper = ax.Scraper(category='physics', date_from='2019-05-01',\n",
    "                     date_until='2019-07-03', t=10,\n",
    "                     filters={'categories':['quant-ph']})\n",
    "\n",
    "output = scraper.scrape()\n",
    "\n",
    "\n",
    "\n",
    "# cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
    "titles = [' '.join(o['title'].split()) for o in output]\n",
    "np.savetxt('titles_ref.csv', np.array(titles), fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alternatively, scrape something else**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "\n",
    "# scrape some interesting quotes\n",
    "url = 'https://raw.githubusercontent.com/akhiltak/inspirational-quotes/master/Quotes.csv'\n",
    "response = urllib.request.urlopen(url).read().decode()\n",
    "quotes = []\n",
    "lines = response.split('\\n')\n",
    "for line in lines[:-1]:\n",
    "    quotes.append(line.split(';')[0].replace(\"\\'\", '').replace('*', '').replace('#', '').replace('%', '').replace('&', ''))\n",
    "    \n",
    "np.savetxt('titles.csv', np.array(quotes[1:]), fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g. could scrape tweets (requires having twitter api credentials)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweetscraper\n",
    "# tweetscraper.get_all_tweets(\"SICKOFWOLVES\") # name of account to scrape\n",
    "tweetscraper.clean_csv(fname='data/wolves_tweets.csv') # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2 - finetune gpt2\n",
    "this code will download gpt2 and finetune it on the file title.csv, generating samples at intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "\n",
    "model_name = \"117M\" # \"355M\" for larger model (it's 1.4 GB)\n",
    "gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/117M/\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              'titles_ref.csv',\n",
    "              model_name=model_name,\n",
    "              steps=1000,\n",
    "              save_every=200,\n",
    "              sample_every=25)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3 - look at the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at some samples\n",
    "the samples are saved to the 'samples' folder by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = 'samples/samples-901'\n",
    "t = open(sample_file, 'r').read()\n",
    "\n",
    "for s in ['endoftext', 'startoftext', '<|', '|>']:\n",
    "    t = t.replace(s, '')\n",
    "for title in t.title().split('\\n')[1:]:\n",
    "    if not title == '':\n",
    "        print('- ' + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating new samples from the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Loading checkpoint checkpoint/run1/model-1000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
     ]
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate one sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Source Separation Via Non-Negative Eigenvector Field Variate Operator\n"
     ]
    }
   ],
   "source": [
    "prefix = 'neural' # None is default\n",
    "text = gpt2.generate(sess,\n",
    "              length=40,\n",
    "              temperature=0.7,\n",
    "              prefix=neural,\n",
    "              nsamples=1,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "             )\n",
    "\n",
    "\n",
    "t = text[0].title()\n",
    "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
    "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate a bunch of samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = gpt2.generate(sess,\n",
    "#               length=40,\n",
    "              temperature=0.7,\n",
    "              prefix=None,\n",
    "              nsamples=100,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "             )\n",
    "\n",
    "\n",
    "t = text[0].title()\n",
    "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
    "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
